{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./../../../text_emotion.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "   category_id  \n",
       "0            0  \n",
       "1            1  \n",
       "2            1  \n",
       "3            2  \n",
       "4            3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting \n",
    "\n",
    "from io import StringIO\n",
    "col = ['tweet_id','sentiment', 'content']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['content'])]\n",
    "# df = df.head(10000)\n",
    "df.columns = ['tweet_id','sentiment', 'content']\n",
    "df['category_id'] = df['sentiment'].factorize()[0]\n",
    "category_id_df = df[['sentiment', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'sentiment']].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "engStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def removeIrrelvantInfo(df,replace_user_reference='<user>',replace_webpage='<url>',replace_hashtag='<hashtag>'):\n",
    "    regex_user_reference = re.compile('@[a-zA-Z0-9]+')\n",
    "    references = []\n",
    "#     for entry in df['content']:\n",
    "#         references.append(regex_references.findall(entry))\n",
    "    df['content'] = df['content'].str.replace(regex_user_reference,replace_user_reference)\n",
    "\n",
    "    regex_websites = re.compile('http://[www.]*[a-zA-Z0-9]+.[a-z]+/[a-zA-Z0-9//]*')\n",
    "    websites = []\n",
    "#     for entry in df['content']:\n",
    "#         regex_websites.findall(entry)\n",
    "    df['content'] = df['content'].str.replace(regex_websites,replace_webpage)\n",
    "    regex_extrachars = re.compile('[\\.\\:]+')\n",
    "    df['content'] = df['content'].str.replace(regex_extrachars,'')\n",
    "    df['content'] = df['content'].str.lower()\n",
    "    df['content'] = df['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in engStopWords]))\n",
    "    df['content'] = df['content'].str.replace(r\"n't\", \" not \")\n",
    "    df['content'] = df['content'].str.replace(r\"?\", \" ? \")\n",
    "    df['content'] = df['content'].str.replace(r\"!\", \" ! \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
      "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
      "\n",
      "   category_id  \n",
      "0            0  \n",
      "1            1  \n",
      "2            1  \n",
      "3            2  \n",
      "4            3  \n",
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  <user> know listenin bad habit earlier started...   \n",
      "1  1956967666     sadness             layin n bed headache ughhhhwaitin call   \n",
      "2  1956967696     sadness                      funeral ceremonygloomy friday   \n",
      "3  1956967789  enthusiasm                         wants hang friends soon !    \n",
      "4  1956968416     neutral     <user> want trade someone houston tickets, one   \n",
      "\n",
      "   category_id  \n",
      "0            0  \n",
      "1            1  \n",
      "2            1  \n",
      "3            2  \n",
      "4            3  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df = removeIrrelvantInfo(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# embeddings_text = open('glove50.txt')\n",
    "WordToVec       = dict()\n",
    "with open('./../../../glove.twitter.27B.50d.txt') as embeddings_text: \n",
    "    for line in embeddings_text:\n",
    "        entry_array = line.split(\" \")\n",
    "        word        = entry_array[0] \n",
    "        word_vector = np.array(entry_array[1:],dtype='float32')\n",
    "        WordToVec[word] = word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78704   0.72151   0.29148  -0.056527  0.31683   0.47172   0.023461\n",
      "  0.69568   0.20782   0.60985  -0.22386   0.7481   -2.6208    0.20117\n",
      " -0.48104   0.12897   0.035239 -0.24486  -0.36088   0.026686  0.28978\n",
      " -0.10698  -0.34621   0.021053  0.54514  -1.0958   -0.274     0.2233\n",
      "  1.0827   -0.029018 -0.84029   0.58619  -0.36511   0.34016   0.89615\n",
      "  0.32757   0.24267   0.68404  -0.34374   0.13583  -2.2162   -0.42537\n",
      "  0.46157   0.88626  -0.22014   0.025599 -0.38615   0.080107 -0.075323\n",
      " -0.61461 ]\n"
     ]
    }
   ],
   "source": [
    "print(WordToVec.get(\"<user>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       194,  79, 329, 197], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 8000\n",
    "tokenizer = Tokenizer(num_words=max_features,lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(df['content'].values)\n",
    "print(tokenizer.document_count)\n",
    "X = tokenizer.texts_to_sequences(df['content'].values)\n",
    "X = pad_sequences(X)\n",
    "X.shape\n",
    "X[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer(stop_words='english',max_features=8000)\n",
    "\n",
    "# X_train_counts = count_vect.fit_transform(df['content'].values)\n",
    "# print(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab=len(tokenizer.word_index.items())\n",
    "# print(vocab)\n",
    "# print(tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_features, 50))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > max_features-1:\n",
    "        continue\n",
    "    else:\n",
    "        embedding_vector = WordToVec.get(word.lower())\n",
    "#         print(embedding_vector)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16140001,  0.22660001,  0.039414  ,  0.33642   , -0.62880999,\n",
       "       -0.1035    ,  0.89679003,  0.72547001, -0.91096002,  0.066873  ,\n",
       "       -0.88328999, -0.10026   , -3.98510003, -0.17095999, -0.77985001,\n",
       "        0.74011999, -0.57288003, -0.23543   , -1.22300005,  0.83262002,\n",
       "        0.15536   ,  0.75707   , -1.06980002,  0.75106001,  0.24905001,\n",
       "        0.73247999, -0.13189   , -0.74685001,  1.55799997, -0.35084999,\n",
       "       -0.41599   , -0.22845   ,  0.0049928 ,  0.57968998,  0.15524   ,\n",
       "        0.41604999,  0.14427   ,  0.23541   ,  0.069321  ,  0.86312997,\n",
       "       -2.20830011, -0.95704001,  0.57643998,  0.55662   ,  0.077452  ,\n",
       "        0.10875   , -0.39614999, -0.81149   , -0.43867999,  0.063696  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 50)            400000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                845       \n",
      "=================================================================\n",
      "Total params: 430,285\n",
      "Trainable params: 30,285\n",
      "Non-trainable params: 400,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 50\n",
    "lstm_out = 64\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(Embedding(max_features, embed_dim, input_length=X.shape[1], weights=[embedding_matrix], trainable=False))\n",
    "# model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(13,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 30)\n",
      "(36000, 30) (36000, 13)\n",
      "(4000, 30) (4000, 13)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['category_id']).values\n",
    "\n",
    "print(X.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 0)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 1000\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      " - 20s - loss: 1.7344 - acc: 0.4036 - val_loss: 1.9286 - val_acc: 0.3680\n",
      "Epoch 2/20\n",
      " - 20s - loss: 1.7321 - acc: 0.4025 - val_loss: 1.9224 - val_acc: 0.3640\n",
      "Epoch 3/20\n",
      " - 20s - loss: 1.7292 - acc: 0.4023 - val_loss: 1.9234 - val_acc: 0.3710\n",
      "Epoch 4/20\n",
      " - 23s - loss: 1.7312 - acc: 0.4024 - val_loss: 1.9225 - val_acc: 0.3750\n",
      "Epoch 5/20\n",
      " - 21s - loss: 1.7290 - acc: 0.4041 - val_loss: 1.9308 - val_acc: 0.3750\n",
      "Epoch 6/20\n",
      " - 22s - loss: 1.7250 - acc: 0.4043 - val_loss: 1.9296 - val_acc: 0.3700\n",
      "Epoch 7/20\n",
      " - 23s - loss: 1.7273 - acc: 0.4037 - val_loss: 1.9272 - val_acc: 0.3740\n",
      "Epoch 8/20\n",
      " - 21s - loss: 1.7242 - acc: 0.4057 - val_loss: 1.9375 - val_acc: 0.3650\n",
      "Epoch 9/20\n",
      " - 21s - loss: 1.7267 - acc: 0.4028 - val_loss: 1.9336 - val_acc: 0.3710\n",
      "Epoch 10/20\n",
      " - 21s - loss: 1.7197 - acc: 0.4051 - val_loss: 1.9364 - val_acc: 0.3680\n",
      "Epoch 11/20\n",
      " - 21s - loss: 1.7215 - acc: 0.4049 - val_loss: 1.9394 - val_acc: 0.3620\n",
      "Epoch 12/20\n",
      " - 22s - loss: 1.7197 - acc: 0.4037 - val_loss: 1.9270 - val_acc: 0.3760\n",
      "Epoch 13/20\n",
      " - 23s - loss: 1.7181 - acc: 0.4084 - val_loss: 1.9452 - val_acc: 0.3700\n",
      "Epoch 14/20\n",
      " - 20s - loss: 1.7219 - acc: 0.4066 - val_loss: 1.9501 - val_acc: 0.3700\n",
      "Epoch 15/20\n",
      " - 20s - loss: 1.7165 - acc: 0.4051 - val_loss: 1.9434 - val_acc: 0.3790\n",
      "Epoch 16/20\n",
      " - 19s - loss: 1.7197 - acc: 0.4035 - val_loss: 1.9521 - val_acc: 0.3600\n",
      "Epoch 17/20\n",
      " - 19s - loss: 1.7149 - acc: 0.4081 - val_loss: 1.9523 - val_acc: 0.3640\n",
      "Epoch 18/20\n",
      " - 19s - loss: 1.7140 - acc: 0.4085 - val_loss: 1.9475 - val_acc: 0.3630\n",
      "Epoch 19/20\n",
      " - 19s - loss: 1.7139 - acc: 0.4075 - val_loss: 1.9528 - val_acc: 0.3610\n",
      "Epoch 20/20\n",
      " - 19s - loss: 1.7136 - acc: 0.4072 - val_loss: 1.9525 - val_acc: 0.3690\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history=model.fit(X_train, Y_train, epochs = 20, batch_size=batch_size, verbose = 2,validation_data=(X_validate,Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 1.88\n",
      "acc: 0.37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Test matrices--\n",
      "accuracy\n",
      "0.36666666666666664\n",
      "f1_score\n",
      "0.18760687320409924\n",
      "Precision_score\n",
      "0.2901805241095606\n",
      "Recall_score\n",
      "0.18986316759934938\n",
      "Confusion_matrix\n",
      "[[  0   0   0  24  16   0   1   0   0   2   0   0   0]\n",
      " [  1  88   0  76 201   2  14   1   5  34   1   1   0]\n",
      " [  0   2   1  22  20   0   4   1   0   9   0   0   0]\n",
      " [  1  38   0 357 149   4  36   5   5  61   1   4   0]\n",
      " [  0  60   0 138 326   1  22   5  11  52   1   7   0]\n",
      " [  0  21   0  44  38   2  11   4   1  30   0   0   0]\n",
      " [  0  10   0  38  30   0 131   3   1  70   0   1   0]\n",
      " [  0   5   0  36  29   1  10   7   0  41   0   0   0]\n",
      " [  0  13   0  15  47   2   1   0  23   5   0   0   0]\n",
      " [  0  13   0  85  62   2  57  10   2 162   0   8   0]\n",
      " [  0   2   0   2   6   0   1   0   0   3   0   0   0]\n",
      " [  0   2   0  30  28   1   8   1   1  24   0   3   0]\n",
      " [  0   0   0   2   3   0   0   1   1   0   0   0   0]]\n",
      "--Train matrices--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.4533333333333333\n",
      "f1_score\n",
      "0.2858105427837289\n",
      "Precision_score\n",
      "0.5612348774082779\n",
      "Recall_score\n",
      "0.26783772636265474\n",
      "Confusion_matrix\n",
      "[[  13   54    0  354  233    3   16   11   14   56    1    3    0]\n",
      " [   1 1553    0  710 1843   15  141   26   70  247    3   17    0]\n",
      " [   0   48    8  244  182    6   27    6    1  153    0    0    0]\n",
      " [   4  286    1 4754 1545   26  316   43   36  703    1   49    0]\n",
      " [   1  595    0 1408 4768   19  240   41   67  444    0   31    0]\n",
      " [   0   99    1  590  647  117  143   24   17  323    0   13    0]\n",
      " [   0  119    1  457  353   14 1822   26   10  641    0   25    0]\n",
      " [   0   44    1  414  285    7   91  227    7  520    0    9    0]\n",
      " [   0  126    0  151  463    3   12    6  388   27    0    4    0]\n",
      " [   0   86    4  891  560   19  555   88   12 2413    0   49    0]\n",
      " [   2   22    0   26   72    0    5    1    5    6   21    2    0]\n",
      " [   0   51    0  380  345    5  106   14    5  256    1  235    0]\n",
      " [   0    7    0   23   46    2    3    3    9    5    0    0    1]]\n"
     ]
    }
   ],
   "source": [
    " Y_pred=model.predict(X_test)\n",
    "print(\"--Test matrices--\")\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1)))\n",
    "print(\"f1_score\")\n",
    "print(f1_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), average=\"macro\"))\n",
    "print(\"Precision_score\")\n",
    "print(precision_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), average=\"macro\"))\n",
    "print(\"Recall_score\")\n",
    "print(recall_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), average=\"macro\"))\n",
    "print(\"Confusion_matrix\")\n",
    "print(confusion_matrix(Y_test.argmax(axis=1),Y_pred.argmax(axis=1)))\n",
    "\n",
    "print(\"--Train matrices--\")\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1)))\n",
    "print(\"f1_score\")\n",
    "print(f1_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1), average=\"macro\"))\n",
    "print(\"Precision_score\")\n",
    "print(precision_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1), average=\"macro\"))\n",
    "print(\"Recall_score\")\n",
    "print(recall_score(Y_train.argmax(axis=1), y_pred_train.argmax(axis=1), average=\"macro\"))\n",
    "print(\"Confusion_matrix\")\n",
    "print(confusion_matrix(Y_train.argmax(axis=1),y_pred_train.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(X,Y): #takes in X and Y as list of text strings\n",
    "    emotions_dict =  {'empty':\"\\033[1;30;47m \", 'sadness':\"\\033[1;37;45m \", 'enthusiasm':\"\\033[1;33;42m \", 'neutral':\"\\033[1;37;40m \", 'worry':\"\\033[1;31;47m \", 'surprise':\"\\033[1;32;43m \", 'love':\"\\033[1;35;46m \", 'fun':\"\\033[1;33;42m \", 'hate':\"\\033[1;31;40m \", 'happiness':\"\\033[1;36;43m \", 'boredom':\"\\033[1;36;47m \", 'relief':\"\\033[1;33;44m \", 'anger':\"\\033[1;47;41m \"}\n",
    "    for i in range(len(X)):\n",
    "        print(emotions_dict[Y[i]]+X[i])\n",
    "        print(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysizeSampleParaSentiment(sample_paragraph):\n",
    "    lines=sample_paragraph.split(\".\")\n",
    "    print(lines)\n",
    "    df_2 = pd.DataFrame(lines, columns=['content'])\n",
    "    df_2=removeIrrelvantInfo(df_2)\n",
    "    print(df_2)\n",
    "    X_sample = tokenizer.texts_to_sequences(df_2['content'].values)\n",
    "    X_pad_sample = pad_sequences(X_sample, maxlen=30)\n",
    "    Y_sample_pred=model.predict(X_pad_sample)\n",
    "    Y_sample_pred_arr=Y_sample_pred.argmax(axis=1)\n",
    "    Y_sample_pred_map=[]\n",
    "    for pred in range(0,len(Y_sample_pred_arr)):\n",
    "        Y_sample_pred_map.append(id_to_category[Y_sample_pred_arr[pred]])\n",
    "    print(colorize(df_2['content'].values,Y_sample_pred_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I feel angry when I see some people throwing trash bags in public places', ' Having picnics at beaches and lakes is unenjoyable because of sickening smells', ' Plastic cups and cases are spattered on the green lawn though a lot of trash tanks spread in the garden']\n",
      "                                             content\n",
      "0  feel angry see people throwing trash bags publ...\n",
      "1  picnics beaches lakes unenjoyable sickening sm...\n",
      "2  plastic cups cases spattered green lawn though...\n",
      "\u001b[1;31;40m feel angry see people throwing trash bags public places\n",
      "hate\n",
      "\u001b[1;37;40m picnics beaches lakes unenjoyable sickening smells\n",
      "neutral\n",
      "\u001b[1;31;47m plastic cups cases spattered green lawn though lot trash tanks spread garden\n",
      "worry\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#angry\n",
    "sample_paragraph='I feel angry when I see some people throwing trash bags in public places. Having picnics at beaches and lakes is unenjoyable because of sickening smells. Plastic cups and cases are spattered on the green lawn though a lot of trash tanks spread in the garden'\n",
    "analysizeSampleParaSentiment(sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a small smile settled onto my tear stained face', ' This is what everyone wanted, well then this is what they were getting', ' The thin blade settled on my skin  it had been there many a time']\n",
      "                                      content\n",
      "0  small smile settled onto tear stained face\n",
      "1               everyone wanted, well getting\n",
      "2           thin blade settled skin many time\n",
      "\u001b[1;35;46m small smile settled onto tear stained face\n",
      "love\n",
      "\u001b[1;37;40m everyone wanted, well getting\n",
      "neutral\n",
      "\u001b[1;31;47m thin blade settled skin many time\n",
      "worry\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#sad\n",
    "sample_paragraph='a small smile settled onto my tear stained face. This is what everyone wanted, well then this is what they were getting. The thin blade settled on my skin  it had been there many a time'\n",
    "analysizeSampleParaSentiment(sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['passion lets us engage deeply in things that matter deeply', ' When we are passionate, we have more energy for our pursuits', ' Passion heightens our awareness and mental acuity']\n",
      "                                             content\n",
      "0  passion lets us engage deeply things matter de...\n",
      "1                        passionate, energy pursuits\n",
      "2          passion heightens awareness mental acuity\n",
      "\u001b[1;37;40m passion lets us engage deeply things matter deeply\n",
      "neutral\n",
      "\u001b[1;37;40m passionate, energy pursuits\n",
      "neutral\n",
      "\u001b[1;31;47m passion heightens awareness mental acuity\n",
      "worry\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#enthusiasm\n",
    "sample_paragraph='passion lets us engage deeply in things that matter deeply. When we are passionate, we have more energy for our pursuits. Passion heightens our awareness and mental acuity'\n",
    "analysizeSampleParaSentiment(sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We live in a society where the lines between good and bad have become blurred', ' And so we worry', ' We worry about a lot of things in our lives, most of which are a result of not fully understanding our situations']\n",
      "                                             content\n",
      "0         live society lines good bad become blurred\n",
      "1                                              worry\n",
      "2  worry lot things lives, result fully understan...\n",
      "\u001b[1;31;47m live society lines good bad become blurred\n",
      "worry\n",
      "\u001b[1;31;47m worry\n",
      "worry\n",
      "\u001b[1;37;40m worry lot things lives, result fully understanding situations\n",
      "neutral\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#worry\n",
    "sample_paragraph='We live in a society where the lines between good and bad have become blurred. And so we worry. We worry about a lot of things in our lives, most of which are a result of not fully understanding our situations'\n",
    "analysizeSampleParaSentiment(sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I feel angry when I see some people throwing trash bags in public places', ' Having picnics at beaches and lakes is unenjoyable because of sickening smells', ' Plastic cups and cases are spattered on the green lawn though a lot of trash tanks spread in the garden']\n",
      "                                             content\n",
      "0  feel angry see people throwing trash bags publ...\n",
      "1  picnics beaches lakes unenjoyable sickening sm...\n",
      "2  plastic cups cases spattered green lawn though...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_paragraph='I feel angry when I see some people throwing trash bags in public places. Having picnics at beaches and lakes is unenjoyable because of sickening smells. Plastic cups and cases are spattered on the green lawn though a lot of trash tanks spread in the garden'\n",
    "lines=sample_paragraph.split(\".\")\n",
    "print(lines)\n",
    "df_2 = pd.DataFrame(lines, columns=['content'])\n",
    "df_2=removeIrrelvantInfo(df_2)\n",
    "print(df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'empty',\n",
       " 1: 'sadness',\n",
       " 2: 'enthusiasm',\n",
       " 3: 'neutral',\n",
       " 4: 'worry',\n",
       " 5: 'surprise',\n",
       " 6: 'love',\n",
       " 7: 'fun',\n",
       " 8: 'hate',\n",
       " 9: 'happiness',\n",
       " 10: 'boredom',\n",
       " 11: 'relief',\n",
       " 12: 'anger'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = tokenizer.texts_to_sequences(df_2['content'].values)\n",
    "X_pad_sample = pad_sequences(X_sample, maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   46,\n",
       "        1758,   25,   77, 4331, 3653, 2689, 1073, 1300],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0, 2730],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 4283, 6623,\n",
       "        4538,  754, 2425,   63,  228, 3653, 2818,  688]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3380840e-02 1.8734878e-01 2.1010756e-03 6.5293901e-02 2.6555657e-01\n",
      "  7.3320055e-03 1.2068903e-02 3.4220149e-03 4.2063975e-01 1.3144353e-03\n",
      "  2.3809189e-03 1.2880785e-03 7.8727845e-03]\n",
      " [4.9127255e-02 1.0452608e-01 1.4385131e-02 4.0374064e-01 1.5300165e-01\n",
      "  8.9618355e-02 2.5624309e-02 9.9109113e-03 3.4930322e-02 9.8344669e-02\n",
      "  5.8089790e-04 1.3724314e-02 2.4855277e-03]\n",
      " [8.3512112e-02 7.4737094e-02 3.5744389e-03 1.8216166e-01 4.1276416e-01\n",
      "  2.2407407e-02 6.5536499e-03 2.5987947e-02 9.3890510e-02 7.8625113e-02\n",
      "  2.6966576e-04 1.1993238e-02 3.5230380e-03]]\n"
     ]
    }
   ],
   "source": [
    "Y_sample_pred=model.predict(X_pad_sample)\n",
    "print(Y_sample_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sample_pred_arr=Y_sample_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sample_pred_map=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate\n",
      "neutral\n",
      "worry\n",
      "['hate', 'neutral', 'worry']\n"
     ]
    }
   ],
   "source": [
    "for pred in range(0,len(Y_sample_pred_arr)):\n",
    "    print(id_to_category[Y_sample_pred_arr[pred]])\n",
    "    Y_sample_pred_map.append(id_to_category[Y_sample_pred_arr[pred]])\n",
    "print(Y_sample_pred_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31;40m feel angry see people throwing trash bags public places\n",
      "hate\n",
      "\u001b[1;37;40m picnics beaches lakes unenjoyable sickening smells\n",
      "neutral\n",
      "\u001b[1;31;47m plastic cups cases spattered green lawn though lot trash tanks spread garden\n",
      "worry\n"
     ]
    }
   ],
   "source": [
    "colorize(df_2['content'].values,Y_sample_pred_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
